{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "\n",
    "data_train_dir='Data/train.txt'\n",
    "data_dev_dir='Data/valid.txt'\n",
    "data_test_dir='Data/test.txt'\n",
    "embedding_dir='Data/glove.6B.100d.txt'\n",
    "worddict_dir='Data/worddict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_dir):\n",
    "    with open(data_dir,'r',encoding='utf-8')as f:\n",
    "        sentences=[]\n",
    "        sentence=[]\n",
    "        labels=[]\n",
    "        label=[]\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    label = []\n",
    "                    sentence = []\n",
    "                continue\n",
    "            line = re.sub('\\n','',line)\n",
    "            splits = line.split(' ')\n",
    "            if len(splits[0])<20:\n",
    "                sentence.append(splits[0])\n",
    "                label.append(splits[-1])\n",
    "        \n",
    "    return {\"sentences\":sentences,\n",
    "            \"labels\":labels\n",
    "           }\n",
    "\n",
    "\n",
    "def build_embedding_map(embedding_dir):\n",
    "    with open(embedding_dir,'r',encoding='utf-8')as f:\n",
    "        embed_map={}\n",
    "        word_embeddings=[]\n",
    "        for lines in f:\n",
    "            lines=lines.strip().split(\" \")\n",
    "            word=lines[0]\n",
    "            if(word) not in embed_map:\n",
    "                embed_map[word]=lines[1:]\n",
    "    return embed_map\n",
    "\n",
    "def build_worddict(data_str,embedding_map):\n",
    "    words=[]\n",
    "    words.extend([\"_PAD_\",\"_OOV_\",\"_BOS_\",\"_EOS_\"])\n",
    "    for sentence in data_str[\"sentences\"]:\n",
    "        words.extend([word.lower() for word in sentence])\n",
    "    for i,embed_word in enumerate(embedding_map):\n",
    "        words.append(embed_word)\n",
    "    word_id={}\n",
    "    for index,word in enumerate(words):\n",
    "        if word not in word_id:\n",
    "            word_id[word]=index\n",
    "    with open(worddict_dir, \"w\",encoding='utf-8') as f:\n",
    "        for word in word_id:\n",
    "            f.write(\"%s\\t%d\\n\"%(word, word_id[word]))\n",
    "            \n",
    "    return word_id\n",
    "\n",
    "def sentence2id(sentence,word_id):\n",
    "    sentence_id=[]\n",
    "    sentence_id.append(word_id[\"_BOS_\"])\n",
    "    for word in sentence:\n",
    "        word=word.lower()\n",
    "        if word in word_id:\n",
    "            sentence_id.append(word_id[word])\n",
    "        else:\n",
    "            sentence_id.append(word_id[\"_OOV_\"])\n",
    "    sentence_id.append(word_id[\"_EOS_\"])\n",
    "    return sentence_id\n",
    "\n",
    "def data2id(data_str,wordid,labelid):\n",
    "    sentences_id=[]\n",
    "    labels_id=[]\n",
    "    char_level_id=[]\n",
    "    for i,seq in enumerate(data_str[\"sentences\"]):\n",
    "        sentence_id=[]\n",
    "        label_id=[]\n",
    "        chars_id=[[95,66,79,83,95]]\n",
    "        #sentence_id\n",
    "        sentence_id.extend(sentence2id(seq,word_id))\n",
    "        #char_level_id\n",
    "        for word in data_str[\"sentences\"][i]:\n",
    "            char_id=[]\n",
    "            for char in word:\n",
    "                char_id.append(ord(char))\n",
    "            chars_id.append(char_id)\n",
    "        chars_id.append([95,69,79,83,95])\n",
    "         #label_id   \n",
    "        label_id.append(labelid[\"_BOS_\"])\n",
    "        for label in data_str[\"labels\"][i]:\n",
    "            label_id.append(labelid[label])\n",
    "        label_id.append(labelid[\"_EOS_\"])\n",
    "        \n",
    "        sentences_id.append(sentence_id)\n",
    "        char_level_id.append(chars_id)\n",
    "        labels_id.append(label_id)\n",
    "    return {\"sentences_id\":sentences_id,\n",
    "            \"char_level_id\":char_level_id,\n",
    "            \"labels_id\":labels_id\n",
    "           }\n",
    "\n",
    "def build_embed_matrix(embed_map,word_id):\n",
    "    vocab_size=len(word_id)\n",
    "    embed_dim=len(embed_map[\"a\"])\n",
    "    matrix=np.zeros((vocab_size,embed_dim))\n",
    "    missed_cnt=0\n",
    "    for index,word in enumerate(word_id):\n",
    "        if word in embed_map:\n",
    "            matrix[index]=embed_map[word]\n",
    "        else:\n",
    "            if word==\"_PAD_\":\n",
    "                continue\n",
    "            else:\n",
    "                missed_cnt+=1\n",
    "                matrix[index]=np.random.normal(size=embed_dim)\n",
    "    print(\"missed word count: %d\"%(missed_cnt)) \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_str=read_data(data_train_dir)\n",
    "data_dev_str=read_data(data_dev_dir)\n",
    "embedding_map=build_embedding_map(embedding_dir)\n",
    "word_id=build_worddict(data_train_str,embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed word count: 2579\n",
      "embedding_matrix size: 402580\n"
     ]
    }
   ],
   "source": [
    "label_idx= {\"_BOS_\": 0,\"B-PER\": 1, \"B-LOC\": 2, \"B-ORG\": 3, \"B-MISC\" : 4,\n",
    "\"I-PER\": 5, \"I-LOC\": 6, \"I-ORG\": 7, \"I-MISC\": 8, \"O\":9 , \"_EOS_\": 10}\n",
    "data_train_id=data2id(data_train_str,word_id,label_idx)\n",
    "data_dev_id=data2id(data_dev_str,word_id,label_idx)\n",
    "embedding_matrix=build_embed_matrix(embedding_map,word_id)\n",
    "print(\"embedding_matrix size: %d\"%len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONLLDataset(Dataset):\n",
    "    def __init__(self,data,max_word_len,max_sentence_len=None):\n",
    "        self.seq_number=len(data[\"sentences_id\"])\n",
    "        self.sentence_lens=[len(seq) for seq in data[\"sentences_id\"]]\n",
    "        self.max_sentence_len=max_sentence_len\n",
    "        self.max_word_len=max_word_len\n",
    "        if self.max_sentence_len==None:\n",
    "            self.max_sentence_len=max(self.sentence_lens)\n",
    "        print(self.seq_number,self.max_sentence_len,self.max_word_len)\n",
    "        self.data={\"sentence\":torch.zeros((self.seq_number,self.max_sentence_len),\n",
    "                                          dtype=torch.long),\n",
    "                   \"label\":torch.zeros((self.seq_number,self.max_sentence_len),\n",
    "                                          dtype=torch.long),\n",
    "                   \"char_level\":torch.zeros((self.seq_number,self.max_sentence_len,\n",
    "                                            self.max_word_len),dtype=torch.long)\n",
    "                  }\n",
    "        for i,seq in enumerate(data[\"sentences_id\"]):\n",
    "            l=len(seq)\n",
    "            self.data[\"sentence\"][i][:l]=torch.tensor(data[\"sentences_id\"][i][:l])\n",
    "            self.data[\"label\"][i][:l]=torch.tensor(data[\"labels_id\"][i][:l])\n",
    "            for j,word in enumerate(data[\"char_level_id\"][i]):\n",
    "                w=len(word)\n",
    "                self.data[\"char_level\"][i][j][:w]=\\\n",
    "                     torch.tensor(data[\"char_level_id\"][i][j][:w])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.seq_number\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return {\"sentence\":self.data[\"sentence\"][index],\n",
    "                \"label\":self.data[\"label\"][index],\n",
    "                \"char_level\":self.data[\"char_level\"][index],\n",
    "                \"length\":min(self.sentence_lens[index], self.max_sentence_len)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "hidden_size=128\n",
    "dropout=0.5\n",
    "lr=0.001\n",
    "epochs=10\n",
    "max_grad_norm=10.0\n",
    "use_gpu=False\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu=True\n",
    "device=torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14026 115 20\n",
      "3250 111 20\n"
     ]
    }
   ],
   "source": [
    "train_dataset=CONLLDataset(data_train_id,20,max_sentence_len=None)\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "dev_dataset=CONLLDataset(data_dev_id,20,max_sentence_len=None)\n",
    "dev_loader=DataLoader(dev_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class char_cnn(nn.Module):\n",
    "    def __init__(self,embed_size,num_filters=None,filter_size=None,\n",
    "                device=\"cpu\"):\n",
    "        super(char_cnn,self).__init__()\n",
    "        self.char_embed_size=embed_size\n",
    "        self.num_filters=num_filters\n",
    "        self.filter_size=filter_size\n",
    "        if self.num_filters==None:\n",
    "            self.num_filters=10\n",
    "        if self.filter_size==None:\n",
    "            self.filter_size=2\n",
    "        print(self.char_embed_size,self.num_filters,self.filter_size)\n",
    "        self.char_embedding=nn.Embedding(256,self.char_embed_size,padding_idx=0)\n",
    "        self.char_embedding.weight.requires_grad = False\n",
    "        self.conv_block=nn.Conv2d(1,self.num_filters,\n",
    "                                  (self.filter_size,103),padding=1)\n",
    "        self.maxpool=nn.MaxPool2d((21,num_filters))\n",
    "        self.dropout=nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "    def forward(self,word_batch):\n",
    "        embeds=self.char_embedding(word_batch)\n",
    "        embeds=embeds.unsqueeze(1)\n",
    "        conv_out=self.conv_block(embeds)\n",
    "        conv_out=conv_out.transpose(1,2)\n",
    "        conv_out=conv_out.reshape(conv_out.size()[0],conv_out.size()[1],-1)\n",
    "        maxpool_out=self.maxpool(conv_out)\n",
    "        #char_out=self.dropout(max_poolout)\n",
    "        return maxpool_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Code widely inspired from:\n",
    "#  https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self,label_idx,device=\"cpu\"):\n",
    "        super(CRF,self).__init__()\n",
    "        self.label_idx=label_idx\n",
    "        self.label_size=len(label_idx)\n",
    "        self.device=device \n",
    "        self.transitions = nn.Parameter(\n",
    "             torch.randn(self.label_size, self.label_size)).to(self.device)\n",
    "        self.transitions.data[label_idx[\"_BOS_\"], :] = -10000.0\n",
    "        self.transitions.data[:, label_idx[\"_EOS_\"]] = -10000.0\n",
    "        \n",
    "            \n",
    "    def _forward_alg(self,feats):# 本质上计算信息传递矩阵\n",
    "        init_alphas=torch.full((1,self.label_size),-10000.).to(self.device)\n",
    "        init_alphas[0][label_idx[\"_BOS_\"]]=0.0\n",
    "        forward_var = init_alphas\n",
    "        for feat in feats:\n",
    "            alphas_t=[]\n",
    "            for next_label in range(self.label_size):\n",
    "                emit_score=feat.view(1,-1).to(self.device)\n",
    "                trans_score=self.transitions[next_label].view(1,-1).to(self.device)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(torch.logsumexp(next_tag_var,dim=1))\n",
    "            forward_var=torch.tensor(alphas_t).view(1,-1).to(self.device)\n",
    "        #print(forward_var.shape)    \n",
    "        terminal_var = forward_var + self.transitions[self.label_idx[\"_EOS_\"]]\n",
    "        alpha = torch.logsumexp(terminal_var,dim=1)\n",
    "        return alpha\n",
    "    \n",
    "    def _sentence_score(self,feats,labels):\n",
    "        score = torch.zeros(1).to(self.device)\n",
    "        tags= torch.tensor(self.label_idx[\"_BOS_\"]).view(1,-1).to(self.device)\n",
    "        tags = torch.cat((tags,labels),dim=1).view(-1)\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score+self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.label_idx[\"_EOS_\"], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def neg_log_likelihood(self,feats,labels):\n",
    "        forward_score=self._forward_alg(feats).to(device)\n",
    "        gold_score=self._sentence_score(feats,labels)\n",
    "        return forward_score-gold_score\n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        \"\"\"维特比解码，给定输入x和相关参数(发射矩阵和转移矩阵)，或者概率最大的标签序列\n",
    "        \"\"\"\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.label_idx[\"_BOS_\"]] = 0\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  \n",
    "            viterbivars_t = []  \n",
    "            for next_tag in range(self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "        terminal_var = forward_var + self.transitions[self.label_idx[\"_EOS_\"]]\n",
    "        best_tag_id = torch.argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.label_idx[\"_BOS_\"]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CNN_CRF(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,label_idx,hidden_size,\n",
    "                 embed_matrix=None,dropout=0.5,device=\"cpu\"):\n",
    "        super(BiLSTM_CNN_CRF,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.label_idx=label_idx\n",
    "        self.label_size=len(label_idx)\n",
    "        self.hidden_size=hidden_size\n",
    "        #self.dropout=dropout\n",
    "        self.device=device\n",
    "        self.word_embedding=nn.Embedding(self.vocab_size,self.embedding_dim,\n",
    "                                        padding_idx=0)\n",
    "        self.word_embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.word_embedding.weight.requires_grad = False\n",
    "        self.char_embedding=char_cnn(embed_size=200,num_filters=10,\n",
    "                                     filter_size=2)\n",
    "        self.encoder=nn.LSTM(input_size=2*self.embedding_dim,\n",
    "                             hidden_size=self.hidden_size//2,\n",
    "                             bidirectional=True\n",
    "                            )\n",
    "        self.hidden=(torch.randn(2, 1, self.hidden_size // 2).to(self.device),\n",
    "                     torch.randn(2, 1, self.hidden_size // 2).to(self.device))\n",
    "        self.hidden2label=nn.Linear(self.hidden_size,self.label_size)\n",
    "        self.crf=CRF(label_idx,self.device)\n",
    "        \n",
    "        \n",
    "    def _get_features(self,sentence,word_batch,sentence_len):\n",
    "        word_batch=word_batch.squeeze(0)[0:sentence_len,:]\n",
    "        word_batch=word_batch.to(self.device)\n",
    "        char_embed=self.char_embedding(word_batch) #sentence_len*1*100\n",
    "        sentence=sentence[:,0:sentence_len]\n",
    "        sentence=sentence.to(self.device)\n",
    "        word_embed=self.word_embedding(sentence) #1*sentence_len*100\n",
    "        word_embed=word_embed.transpose(0,1)    #sentence_len * 1 *100\n",
    "        embed=torch.cat((word_embed,char_embed),dim=2)\n",
    "        bilstm_output,hidden_n=self.encoder(embed,self.hidden)\n",
    "        # bilstm_output= seq_len* 1 * hidden_size\n",
    "        bilstm_output=bilstm_output.squeeze(1)\n",
    "        bilstm_feats=self.hidden2label(bilstm_output)\n",
    "        return bilstm_feats\n",
    "    \n",
    "    def loss_function(self,label,sentence,word_batch,sentence_len):\n",
    "        label=label.to(self.device)\n",
    "        word_batch=word_batch.to(self.device)\n",
    "        sentence_=sentence.to(self.device)\n",
    "        feats=self._get_features(sentence,word_batch,sentence_len)\n",
    "        feats=feats.to(self.device)\n",
    "        loss=self.crf.neg_log_likelihood(feats,label)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self,label_idx,sentence,word_batch,sentence_len):\n",
    "        feats=self._get_features(sentence,word_batch,sentence_len)\n",
    "        score,labels=self.crf._viterbi_decode(feats)\n",
    "        return score,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 10 2\n"
     ]
    }
   ],
   "source": [
    "model=BiLSTM_CNN_CRF(embedding_matrix.shape[0],\n",
    "                     embedding_matrix.shape[1],\n",
    "                    label_idx,hidden_size,embed_matrix=embedding_matrix,\n",
    "                    dropout=dropout,device=device\n",
    "                    ).to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0, loss:10009.0293\n",
      "i:100, loss:10018.4570\n",
      "i:200, loss:10011.3672\n",
      "i:300, loss:10029.8320\n",
      "i:400, loss:10089.9365\n",
      "i:500, loss:10091.7617\n",
      "i:600, loss:10149.0400\n",
      "i:700, loss:10078.2080\n",
      "i:800, loss:10327.1816\n",
      "i:900, loss:10108.0059\n",
      "i:1000, loss:10126.9355\n",
      "i:1100, loss:10148.5762\n",
      "i:1200, loss:10696.4688\n",
      "i:1300, loss:10080.7803\n",
      "i:1400, loss:10088.2148\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        sentence=batch[\"sentence\"]\n",
    "        label=batch[\"label\"]\n",
    "        word_batch=batch[\"char_level\"]\n",
    "        sentence_len=batch[\"length\"]\n",
    "        optimizer.zero_grad()\n",
    "        loss=model.loss_function(label,sentence,word_batch,sentence_len)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        if i % 100==0:\n",
    "            print(\"i:{:d}, loss:{:.4f}\".format(i,loss.item()))\n",
    "    print(\"epoch is:\",epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
