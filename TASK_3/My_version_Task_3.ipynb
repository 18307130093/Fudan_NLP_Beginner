{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import csv\n",
    "import time\n",
    "\n",
    "data_train_dir=\"Data/snli_1.0_train.csv\"\n",
    "data_dev_dir=\"Data/snli_1.0_dev.csv\"\n",
    "data_test_dir=\"Data/snli_1.0_test.csv\"\n",
    "embedding_file_dir=\"Data/glove.6B.300d.txt\"\n",
    "worddict_dir='Data/worddict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(data_dir):\n",
    "    premise=[]\n",
    "    hypothesis=[]\n",
    "    labels=[] \n",
    "    labels_map={\"entailment\":0,\"neutral\":1,\"contradiction\":2}\n",
    "    punct_table = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "    with open(data_dir,'r') as f:\n",
    "        reader=csv.reader(f)\n",
    "        for line in reader:\n",
    "            if line[0] not in labels_map :   #忽略没有label的例子\n",
    "                continue\n",
    "            premise.append(line[5].translate(punct_table).lower())\n",
    "            hypothesis.append(line[6].translate(punct_table).lower())\n",
    "            labels.append(line[0])\n",
    "    del reader\n",
    "    return {\"premise\":premise,\n",
    "            \"hypothesis\":hypothesis,\n",
    "            \"labels\":labels}  \n",
    "\n",
    "def build_worddict(data):\n",
    "    words=[]\n",
    "    words.extend([\"_PAD_\",\"_OOV_\",\"_BOS_\",\"_EOS_\"])\n",
    "    for sentence in data[\"premise\"]:\n",
    "        words.extend(sentence.strip().split(\" \"))\n",
    "    for sentence in data[\"hypothesis\"]:\n",
    "        words.extend(sentence.strip().split(\" \")) \n",
    "    word_id={}\n",
    "    id_word={}\n",
    "    i=0\n",
    "    for index,word in enumerate(words):\n",
    "        if word not in word_id:\n",
    "            word_id[word]=i\n",
    "            id_word[i]=word\n",
    "            i+=1\n",
    "    #保存词典\n",
    "    with open(worddict_dir, \"w\",encoding='utf-8') as f:\n",
    "        for word in word_id:\n",
    "            f.write(\"%s\\t%d\\n\"%(word, word_id[word]))\n",
    "    return word_id,id_word\n",
    "\n",
    "def sentence2idList(sentence,word_id):\n",
    "    ids=[]\n",
    "    ids.append(word_id[\"_BOS_\"])\n",
    "    sentence=sentence.strip().split(\" \")\n",
    "    for word in sentence:\n",
    "        if word not in word_id:\n",
    "            ids.append(word_id[\"_OOV_\"])\n",
    "        else:\n",
    "            ids.append(word_id[word])\n",
    "    ids.append(word_id[\"_EOS_\"])\n",
    "    return ids\n",
    "\n",
    "def data2id(data,word_id):\n",
    "    premise_id=[]\n",
    "    hypothesis_id=[]\n",
    "    labels_id=[] \n",
    "    labels_map={\"entailment\":0,\"neutral\":1,\"contradiction\":2}\n",
    "    for i,label in enumerate(data[\"labels\"]):\n",
    "        if label not in labels_map:   #忽略没有label的例子\n",
    "            continue\n",
    "        premise_id.append(sentence2idList(data[\"premise\"][i],word_id))\n",
    "        hypothesis_id.append(sentence2idList(data[\"hypothesis\"][i],word_id))\n",
    "        labels_id.append(labels_map[label])\n",
    "            \n",
    "    return {\"premise_id\":premise_id,\n",
    "            \"hypothesis_id\":hypothesis_id,\n",
    "            \"labels_id\":labels_id}    \n",
    "\n",
    "def build_embeddings(embedding_file,word_id):   #读取文件存入集合中\n",
    "    embeddings_map={}\n",
    "    with open(embedding_file,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line=line.strip().split()\n",
    "            word=line[0]\n",
    "            if word in word_id:\n",
    "                embeddings_map[word]=line[1:]\n",
    "    #放入矩阵中\n",
    "    words_num = len(word_id)\n",
    "    embedding_dim=len(embeddings_map['a'])\n",
    "    embedding_matrix=np.zeros((words_num,embedding_dim))\n",
    "    #print(words_num,embedding_dim)\n",
    "    missed_cnt=0\n",
    "    for i,word in enumerate(word_id):\n",
    "        if word in embeddings_map:\n",
    "            embedding_matrix[i]=embeddings_map[word]\n",
    "        else:\n",
    "            if word==\"_PAD_\":\n",
    "                continue\n",
    "            else:\n",
    "                missed_cnt+=1\n",
    "                embedding_matrix[i]=np.random.normal(size=embedding_dim)\n",
    "    print(\"missed word count: %d\"%(missed_cnt)) \n",
    "    return embeddings_map,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_str=read_data(data_train_dir)\n",
    "data_dev_str=read_data(data_dev_dir)\n",
    "word_id,id_word=build_worddict(data_train_str)\n",
    "data_train_str[\"hypothesis\"][91381]=\"cannot see picture to discribe\"\n",
    "data_train_str[\"hypothesis\"][91382]=\"there is no picture\"\n",
    "data_train_str[\"hypothesis\"][91383]=\"there is a picture\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed word count: 5994\n",
      "embedding_matrix size: 33268\n"
     ]
    }
   ],
   "source": [
    "data_train_id=data2id(data_train_str,word_id)\n",
    "data_dev_id=data2id(data_dev_str,word_id)\n",
    "embeddings_map,embedding_matrix=build_embeddings(embedding_file_dir,word_id)\n",
    "print(\"embedding_matrix size: %d\"%len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SnliDataSet(Dataset):\n",
    "    def __init__(self,data,max_premise_len=None,max_hypothesis_len=None):\n",
    "        self.num_sequence=len(data[\"premise_id\"])  #创建tensor矩阵的尺寸\n",
    "        self.premise_len=[len(seq) for seq in data[\"premise_id\"]]\n",
    "        self.max_premise_len=max_premise_len\n",
    "        if self.max_premise_len is None:\n",
    "            self.max_premise_len=max(self.premise_len)\n",
    "        \n",
    "        self.hypothesis_len=[len(seq) for seq in data[\"hypothesis_id\"]]\n",
    "        self.max_hypothesis_len=max_hypothesis_len\n",
    "        if max_hypothesis_len is None:\n",
    "            self.max_hypothesis_len=max(self.hypothesis_len)\n",
    "        print(self.num_sequence, self.max_premise_len)\n",
    "        print(self.num_sequence, self.max_hypothesis_len)\n",
    "        #转成tensor，封装到data里\n",
    "        self.data= {\n",
    "            \"premise\":torch.zeros((self.num_sequence,self.max_premise_len),dtype=torch.long),\n",
    "            \"hypothesis\":torch.zeros((self.num_sequence,self.max_hypothesis_len),dtype=torch.long),\n",
    "            \"labels\":torch.zeros(len(data[\"labels_id\"]),dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        for i,premise in enumerate(data[\"premise_id\"]):\n",
    "            l1=len(data[\"premise_id\"][i])\n",
    "            self.data[\"premise\"][i][:l1]=torch.tensor(data[\"premise_id\"][i][:l1])\n",
    "            l2=len(data[\"hypothesis_id\"][i])\n",
    "            self.data[\"hypothesis\"][i][:l2]=torch.tensor(data[\"hypothesis_id\"][i][:l2])\n",
    "            self.data[\"labels\"][i]=data[\"labels_id\"][i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_sequence\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return {\"premises\": self.data[\"premise\"][index],\n",
    "                \"premises_len\":min(self.premise_len[index], self.max_premise_len),\n",
    "                \"hypothesis\":self.data[\"hypothesis\"][index],\n",
    "                \"hypothesis_len\":min(self.hypothesis_len[index], self.max_hypothesis_len),\n",
    "                \"labels\":self.data[\"labels\"][index]   \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367 83\n",
      "549367 66\n",
      "9842 60\n",
      "9842 56\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "patience=3\n",
    "hidden_size=128\n",
    "dropout=0.5\n",
    "num_classes=3\n",
    "lr=0.0001\n",
    "epochs=5\n",
    "max_grad_norm=10.0\n",
    "use_gpu=False\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu=True\n",
    "device=torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "train_data=SnliDataSet(data_train_id,max_premise_len=None,max_hypothesis_len=None)\n",
    "train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "dev_data=SnliDataSet(data_dev_id,max_premise_len=None,max_hypothesis_len=None)\n",
    "dev_loader=DataLoader(dev_data,batch_size=batch_size,shuffle=False)\n",
    "embeddings=torch.tensor(embedding_matrix,dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sort_by_seq_lens(batch, sequences_lengths, descending=True):\n",
    "    sorted_seq_lens, sorting_index =sequences_lengths.sort(0, descending=descending)\n",
    "    sorted_batch = batch.index_select(0, sorting_index)\n",
    "    idx_range = sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n",
    "    _, reverse_mapping = sorting_index.sort(0, descending=False)\n",
    "    restoration_index = idx_range.index_select(0, reverse_mapping)\n",
    "    return sorted_batch, sorted_seq_lens, sorting_index, restoration_index\n",
    "\n",
    "\n",
    "def get_mask(sequences_batch, sequences_lengths):\n",
    "    batch_size = sequences_batch.size()[0]\n",
    "    max_length = torch.max(sequences_lengths)\n",
    "    mask = torch.ones(batch_size, max_length, dtype=torch.float)\n",
    "    mask[sequences_batch[:, :max_length] == 0] = 0.0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def masked_softmax(tensor, mask):\n",
    "    tensor_shape = tensor.size()\n",
    "    reshaped_tensor = tensor.view(-1, tensor_shape[-1])\n",
    "    while mask.dim() < tensor.dim():\n",
    "        mask = mask.unsqueeze(1)\n",
    "    mask = mask.expand_as(tensor).contiguous().float()\n",
    "    reshaped_mask = mask.view(-1, mask.size()[-1])\n",
    "    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-1)\n",
    "    result = result * reshaped_mask\n",
    "    result = result / (result.sum(dim=-1, keepdim=True) + 1e-13)\n",
    "    return result.view(*tensor_shape)\n",
    "\n",
    "\n",
    "def weighted_sum(tensor, weights, mask):\n",
    "    weighted_sum = weights.bmm(tensor)\n",
    "    while mask.dim() < weighted_sum.dim():\n",
    "        mask = mask.unsqueeze(1)\n",
    "    mask = mask.transpose(-1, -2)\n",
    "    mask = mask.expand_as(weighted_sum).contiguous().float()\n",
    "    return weighted_sum * mask\n",
    "\n",
    "\n",
    "def replace_masked(tensor, mask, value):\n",
    "    mask = mask.unsqueeze(1).transpose(2, 1)\n",
    "    reverse_mask = 1.0 - mask\n",
    "    values_to_add = value * reverse_mask\n",
    "    return tensor * mask + values_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Dropout):\n",
    "    def forward(self, sequences_batch):\n",
    "        ones = sequences_batch.data.new_ones(sequences_batch.shape[0],\n",
    "                                             sequences_batch.shape[-1])\n",
    "        dropout_mask = nn.functional.dropout(ones, self.p, self.training,\n",
    "                                             inplace=False)\n",
    "        return dropout_mask.unsqueeze(1) * sequences_batch\n",
    "\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 rnn_type,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers=1,\n",
    "                 bias=True,\n",
    "                 dropout=0.0,\n",
    "                 bidirectional=False):\n",
    "        assert issubclass(rnn_type, nn.RNNBase),\\\n",
    "              \"rnn_type must be a class inheriting from torch.nn.RNNBase\"\n",
    "\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self._encoder = rnn_type(input_size,\n",
    "                                 hidden_size,\n",
    "                                 num_layers=num_layers,\n",
    "                                 bias=bias,\n",
    "                                 batch_first=True,\n",
    "                                 dropout=dropout,\n",
    "                                 bidirectional=bidirectional\n",
    "                                )\n",
    "    def forward(self, sequences_batch, sequences_lengths):\n",
    "        sorted_batch, sorted_lengths, _, restoration_idx =\\\n",
    "            sort_by_seq_lens(sequences_batch, sequences_lengths)\n",
    "        packed_batch = nn.utils.rnn.pack_padded_sequence(sorted_batch,\n",
    "                                                         sorted_lengths,\n",
    "                                                         batch_first=True)\n",
    "\n",
    "        outputs, _ = self._encoder(packed_batch, None)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs,\n",
    "                                                      batch_first=True)\n",
    "        reordered_outputs = outputs.index_select(0, restoration_idx)\n",
    "\n",
    "        return reordered_outputs\n",
    "\n",
    "\n",
    "class SoftmaxAttention(nn.Module):\n",
    "    \n",
    "    def forward(self,\n",
    "                premise_batch,\n",
    "                premise_mask,\n",
    "                hypothesis_batch,\n",
    "                hypothesis_mask):\n",
    "        \n",
    "        # Dot product between premises and hypotheses in each sequence of\n",
    "        # the batch.\n",
    "        similarity_matrix = premise_batch.bmm(hypothesis_batch.transpose(2, 1)\n",
    "                                                              .contiguous())\n",
    "\n",
    "        # Softmax attention weights.\n",
    "        prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)\n",
    "        hyp_prem_attn = masked_softmax(similarity_matrix.transpose(1, 2)\n",
    "                                                        .contiguous(),\n",
    "                                       premise_mask)\n",
    "\n",
    "        # Weighted sums of the hypotheses for the the premises attention,\n",
    "        # and vice-versa for the attention of the hypotheses.\n",
    "        attended_premises = weighted_sum(hypothesis_batch,\n",
    "                                         prem_hyp_attn,\n",
    "                                         premise_mask)\n",
    "        attended_hypotheses = weighted_sum(premise_batch,\n",
    "                                           hyp_prem_attn,\n",
    "                                           hypothesis_mask)\n",
    "\n",
    "        return attended_premises, attended_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the ESIM model presented in the paper \"Enhanced LSTM for\n",
    "    Natural Language Inference\" by Chen et al.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 hidden_size,\n",
    "                 embeddings=None,\n",
    "                 padding_idx=0,\n",
    "                 dropout=0.5,\n",
    "                 num_classes=3,\n",
    "                 device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: The size of the vocabulary of embeddings in the model.\n",
    "            embedding_dim: The dimension of the word embeddings.\n",
    "            hidden_size: The size of all the hidden layers in the network.\n",
    "            embeddings: A tensor of size (vocab_size, embedding_dim) containing\n",
    "                pretrained word embeddings. If None, word embeddings are\n",
    "                initialised randomly. Defaults to None.\n",
    "            padding_idx: The index of the padding token in the premises and\n",
    "                hypotheses passed as input to the model. Defaults to 0.\n",
    "            dropout: The dropout rate to use between the layers of the network.\n",
    "                A dropout rate of 0 corresponds to using no dropout at all.\n",
    "                Defaults to 0.5.\n",
    "            num_classes: The number of classes in the output of the network.\n",
    "                Defaults to 3.\n",
    "            device: The name of the device on which the model is being\n",
    "                executed. Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        super(ESIM, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        self._word_embedding = nn.Embedding(self.vocab_size,\n",
    "                                            self.embedding_dim,\n",
    "                                            padding_idx=padding_idx,\n",
    "                                            _weight=embeddings)\n",
    "\n",
    "        if self.dropout:\n",
    "            self._rnn_dropout = RNNDropout(p=self.dropout)\n",
    "            # self._rnn_dropout = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self._encoding = Seq2SeqEncoder(nn.LSTM,\n",
    "                                        self.embedding_dim,\n",
    "                                        self.hidden_size,\n",
    "                                        bidirectional=True)\n",
    "\n",
    "        self._attention = SoftmaxAttention()\n",
    "\n",
    "        self._projection = nn.Sequential(nn.Linear(4*2*self.hidden_size,\n",
    "                                                   self.hidden_size),\n",
    "                                         nn.ReLU())\n",
    "\n",
    "        self._composition = Seq2SeqEncoder(nn.LSTM,\n",
    "                                           self.hidden_size,\n",
    "                                           self.hidden_size,\n",
    "                                           bidirectional=True)\n",
    "\n",
    "        self._classification = nn.Sequential(nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(2*4*self.hidden_size,\n",
    "                                                       self.hidden_size),\n",
    "                                             nn.Tanh(),\n",
    "                                             nn.Dropout(p=self.dropout),\n",
    "                                             nn.Linear(self.hidden_size,\n",
    "                                                       self.num_classes))\n",
    "\n",
    "        # Initialize all weights and biases in the model.\n",
    "        self.apply(_init_esim_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                premises,\n",
    "                premises_lengths,\n",
    "                hypotheses,\n",
    "                hypotheses_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            premises: A batch of varaible length sequences of word indices\n",
    "                representing premises. The batch is assumed to be of size\n",
    "                (batch, premises_length).\n",
    "            premises_lengths: A 1D tensor containing the lengths of the\n",
    "                premises in 'premises'.\n",
    "            hypothesis: A batch of varaible length sequences of word indices\n",
    "                representing hypotheses. The batch is assumed to be of size\n",
    "                (batch, hypotheses_length).\n",
    "            hypotheses_lengths: A 1D tensor containing the lengths of the\n",
    "                hypotheses in 'hypotheses'.\n",
    "\n",
    "        Returns:\n",
    "            logits: A tensor of size (batch, num_classes) containing the\n",
    "                logits for each output class of the model.\n",
    "            probabilities: A tensor of size (batch, num_classes) containing\n",
    "                the probabilities of each output class in the model.\n",
    "        \"\"\"\n",
    "        premises_mask = get_mask(premises, premises_lengths).to(self.device)\n",
    "        hypotheses_mask = get_mask(hypotheses, hypotheses_lengths)\\\n",
    "            .to(self.device)\n",
    "\n",
    "        embedded_premises = self._word_embedding(premises)\n",
    "        embedded_hypotheses = self._word_embedding(hypotheses)\n",
    "\n",
    "        if self.dropout:\n",
    "            embedded_premises = self._rnn_dropout(embedded_premises)\n",
    "            embedded_hypotheses = self._rnn_dropout(embedded_hypotheses)\n",
    "\n",
    "        encoded_premises = self._encoding(embedded_premises,\n",
    "                                          premises_lengths)\n",
    "        encoded_hypotheses = self._encoding(embedded_hypotheses,\n",
    "                                            hypotheses_lengths)\n",
    "\n",
    "        attended_premises, attended_hypotheses =\\\n",
    "            self._attention(encoded_premises, premises_mask,\n",
    "                            encoded_hypotheses, hypotheses_mask)\n",
    "\n",
    "        enhanced_premises = torch.cat([encoded_premises,\n",
    "                                       attended_premises,\n",
    "                                       encoded_premises - attended_premises,\n",
    "                                       encoded_premises * attended_premises],\n",
    "                                      dim=-1)\n",
    "        enhanced_hypotheses = torch.cat([encoded_hypotheses,\n",
    "                                         attended_hypotheses,\n",
    "                                         encoded_hypotheses -\n",
    "                                         attended_hypotheses,\n",
    "                                         encoded_hypotheses *\n",
    "                                         attended_hypotheses],\n",
    "                                        dim=-1)\n",
    "\n",
    "        projected_premises = self._projection(enhanced_premises)\n",
    "        projected_hypotheses = self._projection(enhanced_hypotheses)\n",
    "\n",
    "        if self.dropout:\n",
    "            projected_premises = self._rnn_dropout(projected_premises)\n",
    "            projected_hypotheses = self._rnn_dropout(projected_hypotheses)\n",
    "\n",
    "        v_ai = self._composition(projected_premises, premises_lengths)\n",
    "        v_bj = self._composition(projected_hypotheses, hypotheses_lengths)\n",
    "\n",
    "        v_a_avg = torch.sum(v_ai * premises_mask.unsqueeze(1)\n",
    "                                                .transpose(2, 1), dim=1)\\\n",
    "            / torch.sum(premises_mask, dim=1, keepdim=True)\n",
    "        v_b_avg = torch.sum(v_bj * hypotheses_mask.unsqueeze(1)\n",
    "                                                  .transpose(2, 1), dim=1)\\\n",
    "            / torch.sum(hypotheses_mask, dim=1, keepdim=True)\n",
    "\n",
    "        v_a_max, _ = replace_masked(v_ai, premises_mask, -1e7).max(dim=1)\n",
    "        v_b_max, _ = replace_masked(v_bj, hypotheses_mask, -1e7).max(dim=1)\n",
    "\n",
    "        v = torch.cat([v_a_avg, v_a_max, v_b_avg, v_b_max], dim=1)\n",
    "\n",
    "        logits = self._classification(v)\n",
    "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        return logits, probabilities\n",
    "\n",
    "\n",
    "def _init_esim_weights(module):\n",
    "    \"\"\"\n",
    "    Initialise the weights of the ESIM model.\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight.data)\n",
    "        nn.init.constant_(module.bias.data, 0.0)\n",
    "\n",
    "    elif isinstance(module, nn.LSTM):\n",
    "        nn.init.xavier_uniform_(module.weight_ih_l0.data)\n",
    "        nn.init.orthogonal_(module.weight_hh_l0.data)\n",
    "        nn.init.constant_(module.bias_ih_l0.data, 0.0)\n",
    "        nn.init.constant_(module.bias_hh_l0.data, 0.0)\n",
    "        hidden_size = module.bias_hh_l0.data.shape[0] // 4\n",
    "        module.bias_hh_l0.data[hidden_size:(2*hidden_size)] = 1.0\n",
    "\n",
    "        if (module.bidirectional):\n",
    "            nn.init.xavier_uniform_(module.weight_ih_l0_reverse.data)\n",
    "            nn.init.orthogonal_(module.weight_hh_l0_reverse.data)\n",
    "            nn.init.constant_(module.bias_ih_l0_reverse.data, 0.0)\n",
    "            nn.init.constant_(module.bias_hh_l0_reverse.data, 0.0)\n",
    "            module.bias_hh_l0_reverse.data[hidden_size:(2*hidden_size)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESIM(embeddings.shape[0],\n",
    "             embeddings.shape[1],\n",
    "             hidden_size,\n",
    "             embeddings=embeddings,\n",
    "             dropout=dropout,\n",
    "             num_classes=num_classes,\n",
    "             device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备训练\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"max\",factor=0.5,patience=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrectNum(probs, targets):\n",
    "    _, out_classes = probs.max(dim=1)\n",
    "    correct = (out_classes == targets).sum()\n",
    "    return correct.item()\n",
    "\n",
    "def train(model, data_loader, optimizer, criterion, max_gradient_norm):\n",
    "    model.train()\n",
    "    device=model.device\n",
    "    \n",
    "    time_epoch_start= time.time()\n",
    "    running_loss=0 \n",
    "    correct_cnt=0\n",
    "    batch_cnt=0\n",
    "    \n",
    "    for index,batch in enumerate(data_loader):\n",
    "        time_batch_start=time.time()\n",
    "        #从data_loader中取出数据\n",
    "        premises=batch[\"premises\"].to(device)\n",
    "        premises_len=batch[\"premises_len\"].to(device)\n",
    "        hypothesis=batch[\"hypothesis\"].to(device)\n",
    "        hypothesis_len=batch[\"hypothesis_len\"].to(device)\n",
    "        labels=batch[\"labels\"].to(device)\n",
    "        #梯度置0\n",
    "        optimizer.zero_grad()\n",
    "        #正向传播\n",
    "        logits,probs=model(premises,premises_len,hypothesis,hypothesis_len)\n",
    "        #求损失，反向传播，梯度裁剪，更新权重\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+=loss.item()\n",
    "        correct_cnt+=getCorrectNum(probs,labels)\n",
    "        batch_cnt+=1\n",
    "        if index%100==0:\n",
    "            print(\"Training  ------>   Batch count: {:d}/{:d},  batch time: {:.4f}s,  batch average loss: {:.4f}\"\n",
    "              .format(batch_cnt,len(data_loader),time.time()-time_batch_start, running_loss/(index+1)))\n",
    "        del premises\n",
    "        del premises_len\n",
    "        del hypothesis\n",
    "        del hypothesis_len\n",
    "        del labels\n",
    "        \n",
    "    epoch_time = time.time() - time_epoch_start\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    epoch_accuracy = correct_cnt / len(data_loader.dataset) \n",
    "    return epoch_time,epoch_loss,epoch_accuracy\n",
    "\n",
    "def validate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    device=model.device\n",
    "    \n",
    "    time_epoch_start= time.time()\n",
    "    running_loss=0 \n",
    "    correct_cnt=0\n",
    "    batch_cnt=0\n",
    "\n",
    "    for index,batch in enumerate(data_loader):\n",
    "        time_batch_start= time.time()\n",
    "        #从data_loader中取出数据\n",
    "        premises=batch[\"premises\"].to(device)\n",
    "        premises_len=batch[\"premises_len\"].to(device)\n",
    "        hypothesis=batch[\"hypothesis\"].to(device)\n",
    "        hypothesis_len=batch[\"hypothesis_len\"].to(device)\n",
    "        labels=batch[\"labels\"].to(device)\n",
    "        \n",
    "        #正向传播\n",
    "        logits,probs=model(premises,premises_len,hypothesis,hypothesis_len)\n",
    "\n",
    "        #求损失\n",
    "        loss = criterion(logits, labels)\n",
    "        running_loss+=loss.item()\n",
    "        correct_cnt+=getCorrectNum(probs,labels)\n",
    "        batch_cnt+=1\n",
    "        if index%100==0:\n",
    "            print(\"Training  ------>   Batch count: {:d}/{:d},  batch time: {:.4f}s,  batch average loss: {:.4f}\"\n",
    "              .format(batch_cnt,len(data_loader),time.time()-time_batch_start, running_loss/(index+1)))\n",
    "        del premises\n",
    "        del premises_len\n",
    "        del hypothesis\n",
    "        del hypothesis_len\n",
    "        del labels\n",
    "        \n",
    "    epoch_time = time.time() - time_epoch_start\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    epoch_accuracy = correct_cnt / len(data_loader.dataset) \n",
    "    return epoch_time,epoch_loss,epoch_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练过程中的参数\n",
    "best_score=0.0\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "patience_cnt=0\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #训练\n",
    "    print(\"-\"*50,\"Training epoch %d\"%(epoch),\"-\"*50)\n",
    "    epoch_time,epoch_loss,epoch_accuracy =train(model,train_loader,optimizer,criterion,max_grad_norm)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(\"Training time: {:.4f}s, loss :{:.4f}, accuracy: {:.4f}%\".format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n",
    "    \n",
    "    #验证\n",
    "    print(\"-\"*50,\"Validating epoch %d\"%(epoch),\"-\"*50)\n",
    "    epoch_time_dev, epoch_loss_dev, epoch_accuracy_dev = validate(model,dev_loader,criterion)\n",
    "    valid_losses.append(epoch_loss_dev)\n",
    "    print(\"Validating time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%\\n\".format(epoch_time_dev, epoch_loss_dev, (epoch_accuracy_dev*100)))\n",
    "    \n",
    "    #更新学习率\n",
    "    scheduler.step(epoch_accuracy)\n",
    "    \n",
    "    #early stoping\n",
    "    if epoch_accuracy_dev< best_score:\n",
    "        patience_cnt+=1\n",
    "    else:\n",
    "        best_score=epoch_accuracy_dev\n",
    "        patience_cnt=0\n",
    "    if patience_cnt>=patience:\n",
    "            print(\"-\"*50,\"Early stopping\",\"-\"*50)\n",
    "            break\n",
    "        \n",
    "    #每个epoch都保存模型\n",
    "    torch.save({\"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"best_score\": best_score,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"valid_losses\": valid_losses},\n",
    "               \"model_train_dir_\"+str(epoch)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}