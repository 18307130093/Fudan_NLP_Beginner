{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "EPOCH2 = 20\n",
    "num_class = 5\n",
    "sentence_maxlength = 60\n",
    "EMBEDDING_SIZE = 100\n",
    "filter_size = [2, 3, 4, 5]\n",
    "num_filter = 128\n",
    "dropout_rate = 0.5\n",
    "\n",
    "params = {'batch_size' : 64,\n",
    "          'shuffle' : True\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    file = pd.read_csv(path, sep='\\t', header=0, index_col='PhraseId')\n",
    "    file = np.array(file)\n",
    "    num = file.shape[0]\n",
    "    for i in range(num):\n",
    "        file[i][1] = file[i][1].lower()\n",
    "    return file, num\n",
    "    \n",
    "def read_pretrain_vector(path):\n",
    "    emb = []\n",
    "    vocab = []\n",
    "    dic = {}\n",
    "    index = 0\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            row = line.strip().split()\n",
    "            emb.append(row[1:])\n",
    "            vocab.append(row[0])\n",
    "            dic[row[0]] = index\n",
    "            index += 1\n",
    "    return vocab, emb, dic\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, datas, labels):\n",
    "        self.datas = datas\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sentence, label = self.datas[index], self.labels[index]\n",
    "        return sentence, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    \n",
    "    \n",
    "def make_dicts(text):\n",
    "    # text : list of sentences\n",
    "    dicts = set()\n",
    "    for sentence in text:\n",
    "        sentence_list = sentence.split()\n",
    "        for word in sentence_list:\n",
    "            dicts.add(word)\n",
    "    return dicts\n",
    "        \n",
    "\n",
    "def one_hot_vector(value, num):\n",
    "    out = np.zeros(num)\n",
    "    out[value] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_data(dataset, dictts, train=True):\n",
    "    num = dataset.shape[0]\n",
    "    out = []\n",
    "    label = []\n",
    "    for i in range(num):\n",
    "        sentence = dataset[i][1]\n",
    "        embed = []\n",
    "        sentence_list = sentence.split()\n",
    "        for word in sentence_list:\n",
    "            embed.append(dictts[word])\n",
    "        if len(embed) <= sentence_maxlength:\n",
    "            for j in range(sentence_maxlength - len(embed)):\n",
    "                embed.append(0)\n",
    "        else:\n",
    "            embed = embed[:sentence_maxlength]\n",
    "        out.append(embed)\n",
    "        if train == True:\n",
    "            label.append(dataset[i][2])\n",
    "        \n",
    "    return out, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.num_filters_total = num_filter * len(filter_size)\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "             nn.Conv2d(1, num_filter, (filter_size[0], EMBEDDING_SIZE), bias=True),\n",
    "             nn.Dropout(dropout_rate),\n",
    "             nn.ReLU(),\n",
    "             nn.MaxPool2d((sentence_maxlength - filter_size[0] + 1, 1)),\n",
    "                                         )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filter, (filter_size[1], EMBEDDING_SIZE), bias=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((sentence_maxlength - filter_size[1] + 1, 1)),\n",
    "                                        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filter, (filter_size[2], EMBEDDING_SIZE), bias=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((sentence_maxlength - filter_size[2] + 1, 1)),\n",
    "                                    )\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filter, (filter_size[3], EMBEDDING_SIZE), bias=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((sentence_maxlength - filter_size[3] + 1, 1)),\n",
    "                                    )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(self.num_filters_total, num_class)\n",
    "        \n",
    "# input : [batch_size, 1, height=sentence_maxlength, width=EMBEDDING_SIZE]             \n",
    "    def forward(self, x):\n",
    "        pool_out = []\n",
    "        conv_out1 = self.conv_block1(x) \n",
    "        pool1 = conv_out1.permute(0, 3, 2, 1)   \n",
    "        #[batch_size, num_filters, height=1, width=1]\n",
    "        pool_out.append(pool1)\n",
    "        conv_out2 = self.conv_block2(x) \n",
    "        pool2 = conv_out2.permute(0, 3, 2, 1)  \n",
    "        pool_out.append(pool2)\n",
    "        conv_out3 = self.conv_block3(x) \n",
    "        pool3 = conv_out3.permute(0, 3, 2, 1)   \n",
    "        pool_out.append(pool3)\n",
    "        conv_out4 = self.conv_block4(x) \n",
    "        pool4 = conv_out4.permute(0, 3, 2, 1)   \n",
    "        pool_out.append(pool4)\n",
    "        h_pool = torch.cat(pool_out, 3)\n",
    "        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total])\n",
    "        h_pool_flat = self.dropout(h_pool_flat)\n",
    "        out = self.fc(h_pool_flat)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, num_train = load_data('train.tsv')\n",
    "test_data, num_test = load_data('test.tsv')\n",
    "glove_vocab, glove_emb, dic = read_pretrain_vector('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = []\n",
    "test_sentence = []\n",
    "for i in range(num_train):\n",
    "        train_sentence.append(train_data[i][1])\n",
    "for i in range(num_test):\n",
    "        test_sentence.append(test_data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = make_dicts(train_sentence)\n",
    "test_dict = make_dicts(test_sentence)\n",
    "tot_dict = train_dict | test_dict\n",
    "tot_dict = sorted(list(tot_dict))\n",
    "tot_num_word = len(tot_dict)\n",
    "dicts = {w : i for i, w in enumerate(tot_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_weight = np.zeros([tot_num_word, EMBEDDING_SIZE])\n",
    "for i in range(tot_num_word):\n",
    "        word = tot_dict[i]\n",
    "        if word in glove_vocab:\n",
    "            num = dic[word]\n",
    "            vector = glove_emb[num]\n",
    "            vector = np.array(vector)\n",
    "            pretrain_weight[i] = vector\n",
    "        else:\n",
    "            pretrain_weight[i] = np.random.rand(1, EMBEDDING_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whetted'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_weight.shape[0]\n",
    "tot_dict[19020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_emb, train_label = make_data(train_data, dicts, train=True)\n",
    "    test_emb, _ = make_data(test_data, dicts, train=False)\n",
    "    train_emb = np.array(train_emb)\n",
    "    train_label = np.array(train_label)\n",
    "    test_emb = np.array(test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_emb, train_label)\n",
    "train_generator = data.DataLoader(train_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x27c543f5198>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1,  loss:1.341\n",
      "Epoch:1,  loss:1.160\n",
      "Epoch:1,  loss:1.310\n",
      "Epoch:2,  loss:1.188\n",
      "Epoch:2,  loss:1.160\n",
      "Epoch:2,  loss:1.002\n",
      "Epoch:3,  loss:1.193\n",
      "Epoch:3,  loss:1.095\n",
      "Epoch:3,  loss:1.344\n",
      "Epoch:4,  loss:1.032\n",
      "Epoch:4,  loss:1.080\n",
      "Epoch:4,  loss:1.015\n",
      "Epoch:5,  loss:0.987\n",
      "Epoch:5,  loss:1.076\n",
      "Epoch:5,  loss:1.262\n",
      "Epoch:6,  loss:1.123\n",
      "Epoch:6,  loss:1.059\n",
      "Epoch:6,  loss:1.012\n",
      "Epoch:7,  loss:0.858\n",
      "Epoch:7,  loss:1.141\n",
      "Epoch:7,  loss:1.002\n",
      "Epoch:8,  loss:0.980\n",
      "Epoch:8,  loss:1.034\n",
      "Epoch:8,  loss:0.985\n",
      "Epoch:9,  loss:1.250\n",
      "Epoch:9,  loss:1.019\n",
      "Epoch:9,  loss:1.155\n",
      "Epoch:10,  loss:1.213\n",
      "Epoch:10,  loss:1.056\n",
      "Epoch:10,  loss:0.955\n",
      "Epoch:11,  loss:1.014\n",
      "Epoch:11,  loss:0.889\n",
      "Epoch:11,  loss:1.175\n",
      "Epoch:12,  loss:1.156\n",
      "Epoch:12,  loss:1.139\n",
      "Epoch:12,  loss:1.350\n",
      "Epoch:13,  loss:0.954\n",
      "Epoch:13,  loss:1.214\n",
      "Epoch:13,  loss:1.078\n",
      "Epoch:14,  loss:1.314\n",
      "Epoch:14,  loss:1.153\n",
      "Epoch:14,  loss:0.830\n",
      "Epoch:15,  loss:1.106\n",
      "Epoch:15,  loss:1.085\n",
      "Epoch:15,  loss:1.309\n",
      "Epoch:16,  loss:0.955\n",
      "Epoch:16,  loss:1.109\n",
      "Epoch:16,  loss:0.866\n",
      "Epoch:17,  loss:1.003\n",
      "Epoch:17,  loss:0.898\n",
      "Epoch:17,  loss:0.973\n",
      "Epoch:18,  loss:1.053\n",
      "Epoch:18,  loss:1.175\n",
      "Epoch:18,  loss:0.969\n",
      "Epoch:19,  loss:1.093\n",
      "Epoch:19,  loss:1.062\n",
      "Epoch:19,  loss:1.018\n",
      "Epoch:20,  loss:1.223\n",
      "Epoch:20,  loss:1.049\n",
      "Epoch:20,  loss:1.063\n"
     ]
    }
   ],
   "source": [
    "net = CNN().to(device)\n",
    "weight = torch.FloatTensor(pretrain_weight)\n",
    "embedd = nn.Embedding.from_pretrained(weight).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.003, weight_decay=0.001)\n",
    "net.train(mode=True)\n",
    "for epoch in range(EPOCH2):\n",
    "        for i, (batch_train,batch_label) in enumerate(train_generator): \n",
    "            #(batch_train, batch_label)\n",
    "            batch_train = batch_train.long()\n",
    "            batch_train = batch_train.to(device)\n",
    "            batch_train = embedd(batch_train)\n",
    "            batch_train = batch_train.unsqueeze(1)\n",
    "            batch_label = batch_label.to(device)\n",
    "            out = net(batch_train)\n",
    "            batch_label = batch_label.long()\n",
    "            loss = criterion(out, batch_label)\n",
    "            if i%1000==0:\n",
    "                print('Epoch:{},  loss:{:.3f}'.format(epoch + 1,  loss.item()))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "k = num_test // 10000 + 1\n",
    "for i in range(k):\n",
    "        begin = i * 10000\n",
    "        end = (i + 1) * 10000\n",
    "        if end > num_test:\n",
    "            end = num_test\n",
    "        mini_test = test_emb[begin : end]\n",
    "        mini_test = torch.from_numpy(mini_test).long()\n",
    "        mini_test = mini_test.to(device)\n",
    "        mini_test = embedd(mini_test)\n",
    "        mini_test = mini_test.unsqueeze(1)\n",
    "        result = net(mini_test)\n",
    "        result = result.cpu()\n",
    "        result = result.detach()\n",
    "        result = F.softmax(result,dim=1)\n",
    "        result_ = np.argmax(result, axis=1)\n",
    "        result_ = list(result_)\n",
    "        num_list = list(range(156061 + begin, 156061 + end))\n",
    "        #print(num_list)\n",
    "        dataframe = pd.DataFrame({'PhraseId':num_list, 'Sentiment':result_})\n",
    "        print(dataframe)\n",
    "        #dataframe.to_csv('q2_L2_dropout_textcnn_mySubmission%d.csv' \n",
    "        #                 % (i + 1), index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
