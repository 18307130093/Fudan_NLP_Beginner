{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "\n",
    "Epoch = 100\n",
    "Dropout = 0.5\n",
    "Hidden_size = 512\n",
    "Embedding_size = 512\n",
    "Batch_size = 64\n",
    "len_seq = 20\n",
    "LR = 0.006\n",
    "\n",
    "params = {'batch_size' : Batch_size,\n",
    "          'shuffle' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    tot = ''\n",
    "    with open(path, 'r', encoding='UTF-8') as file:\n",
    "        data = file.readlines()\n",
    "        for i in range(len(data)):\n",
    "            tot = tot + data[i]\n",
    "    return tot\n",
    "\n",
    "\n",
    "def dicts(raw):\n",
    "    char_list = set()\n",
    "    for char in raw:\n",
    "        char_list.add(char)\n",
    "    char_list = sorted(list(char_list))\n",
    "    char2num = {char : i for i, char in enumerate(char_list)}\n",
    "    tot_num = len(char_list)\n",
    "    word_num_freq = np.zeros([tot_num], dtype=np.int16)\n",
    "    for word in raw:\n",
    "        word_num_freq[char2num[word]] += 1\n",
    "    return char_list, char2num, tot_num, word_num_freq\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(num, tot_len):\n",
    "    out = np.zeros(tot_len)\n",
    "    out[num] = 1\n",
    "    return out\n",
    "\n",
    "def sentence_char2num(x, char2num):\n",
    "    return [char2num[char] for char in x]\n",
    "\n",
    "def sentence_num2char(x, char_list):\n",
    "    return [char_list[num] for num in x]\n",
    "\n",
    "def softmax(y):\n",
    "    ave = np.mean(y)\n",
    "    y = y - ave\n",
    "    tot = np.sum(np.exp(y))\n",
    "    out = np.exp(y) / tot\n",
    "    return out\n",
    "\n",
    "def random_choice(x, k=5):\n",
    "    index = np.argsort(-x)\n",
    "    character = index[:k]\n",
    "    prob = x[character]\n",
    "    prob = prob / prob.sum()\n",
    "    out = np.random.choice(character, size=1, p=prob)\n",
    "    return out\n",
    "\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, datas):\n",
    "        self.datas = datas\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        train = self.datas[index, :]\n",
    "        label = np.zeros(train.shape, dtype=np.int16)\n",
    "        label[:-1], label[-1] = train[1:], train[0]\n",
    "        return train, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.datas.shape[0]\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, dropout, num_class):\n",
    "        super(RNN, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_class = num_class\n",
    "        self.embed = nn.Embedding(self.num_class, self.emb_size)\n",
    "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, self.num_layers, bias=True, dropout=self.dropout)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_class, bias=True)\n",
    "        \n",
    "    def forward(self, x, init=None):\n",
    "        # x : batch, len_seq, num_class\n",
    "        if init == None:\n",
    "            h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)\n",
    "        else:\n",
    "            (h0, c0) = init\n",
    "            \n",
    "        word_emb = self.embed(x)\n",
    "        change = word_emb.permute(1, 0, 2)\n",
    "        out, (hn, cn) = self.lstm(change, (h0, c0))\n",
    "        # out : Time_step, batch, hidden_size\n",
    "        ts, ba, hd = out.shape\n",
    "        out = out.view(ts * ba, hd)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(ts, ba, -1)\n",
    "        out = out.permute(1, 0, 2).contiguous() \n",
    "        # out : batch, Time_step, num_class\n",
    "        out = out.view(-1, out.shape[2])\n",
    "        return out, (hn, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_data('poetryFromTang.txt')\n",
    "raw_data = raw_data.replace('\\n', '')\n",
    "raw_data = raw_data.replace('\\ufeff', '')\n",
    "raw_data = raw_data.replace('；', ' ')\n",
    "raw_data = raw_data.replace('，', ' ')\n",
    "raw_data = raw_data.replace('。', ' ')\n",
    "raw_data = ' '.join(raw_data.split())\n",
    "char_list, char2num, tot_num, word_num_freq = dicts(raw_data)\n",
    "word_freq = []\n",
    "for i in range(word_num_freq.shape[0]):\n",
    "    word_freq.append((char_list[i], word_num_freq[i]))\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "high_fre_word = [word_freq[i][0] for i in range(len(word_freq)) if word_freq[i][1] > 1]\n",
    "high_freq = len(high_fre_word)\n",
    "\n",
    "corpus = raw_data\n",
    "num_seq = len(corpus) // len_seq\n",
    "corpus = corpus[:num_seq * len_seq]\n",
    "\n",
    "\n",
    "dataset = np.zeros([num_seq, len_seq], dtype=np.int16)\n",
    "for i in range(num_seq):\n",
    "    dataset[i] = np.array([char2num[char] for char in raw_data[i * len_seq : (i + 1) * len_seq]])\n",
    "\n",
    "\n",
    "train_set = CustomDataset(dataset)\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "\n",
    "net = RNN(emb_size=Embedding_size, hidden_size=Hidden_size, num_layers=3, dropout=Dropout, num_class=tot_num).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Step:1, loss:7.831\n",
      "perplexity:1290.666\n",
      "Epoch:2, Step:1, loss:6.501\n",
      "perplexity:692.683\n",
      "Epoch:3, Step:1, loss:6.364\n",
      "perplexity:571.558\n",
      "Epoch:4, Step:1, loss:6.224\n",
      "perplexity:518.006\n",
      "Epoch:5, Step:1, loss:6.033\n",
      "perplexity:474.226\n",
      "Epoch:6, Step:1, loss:6.090\n",
      "perplexity:441.707\n",
      "Epoch:7, Step:1, loss:6.005\n",
      "perplexity:409.275\n",
      "Epoch:8, Step:1, loss:5.912\n",
      "perplexity:381.288\n",
      "Epoch:9, Step:1, loss:5.820\n",
      "perplexity:347.070\n",
      "Epoch:10, Step:1, loss:5.749\n",
      "perplexity:319.460\n",
      "Epoch:11, Step:1, loss:5.676\n",
      "perplexity:296.171\n",
      "Epoch:12, Step:1, loss:5.591\n",
      "perplexity:275.077\n",
      "Epoch:13, Step:1, loss:5.501\n",
      "perplexity:252.803\n",
      "Epoch:14, Step:1, loss:5.377\n",
      "perplexity:234.542\n",
      "Epoch:15, Step:1, loss:5.299\n",
      "perplexity:212.587\n",
      "Epoch:16, Step:1, loss:5.222\n",
      "perplexity:197.301\n",
      "Epoch:17, Step:1, loss:5.161\n",
      "perplexity:179.377\n",
      "Epoch:18, Step:1, loss:5.047\n",
      "perplexity:166.853\n",
      "Epoch:19, Step:1, loss:4.975\n",
      "perplexity:149.910\n",
      "Epoch:20, Step:1, loss:4.848\n",
      "perplexity:135.159\n",
      "Epoch:21, Step:1, loss:4.741\n",
      "perplexity:124.188\n",
      "Epoch:22, Step:1, loss:4.707\n",
      "perplexity:111.919\n",
      "Epoch:23, Step:1, loss:4.557\n",
      "perplexity:101.311\n",
      "Epoch:24, Step:1, loss:4.450\n",
      "perplexity:91.291\n",
      "Epoch:25, Step:1, loss:4.278\n",
      "perplexity:82.637\n",
      "Epoch:26, Step:1, loss:4.319\n",
      "perplexity:73.588\n",
      "Epoch:27, Step:1, loss:4.116\n",
      "perplexity:68.615\n",
      "Epoch:28, Step:1, loss:3.906\n",
      "perplexity:60.614\n",
      "Epoch:29, Step:1, loss:3.960\n",
      "perplexity:54.981\n",
      "Epoch:30, Step:1, loss:3.844\n",
      "perplexity:49.075\n",
      "Epoch:31, Step:1, loss:3.739\n",
      "perplexity:44.748\n",
      "Epoch:32, Step:1, loss:3.588\n",
      "perplexity:40.995\n",
      "Epoch:33, Step:1, loss:3.648\n",
      "perplexity:36.316\n",
      "Epoch:34, Step:1, loss:3.300\n",
      "perplexity:32.467\n",
      "Epoch:35, Step:1, loss:3.319\n",
      "perplexity:29.247\n",
      "Epoch:36, Step:1, loss:3.166\n",
      "perplexity:27.114\n",
      "Epoch:37, Step:1, loss:3.190\n",
      "perplexity:24.419\n",
      "Epoch:38, Step:1, loss:2.996\n",
      "perplexity:22.452\n",
      "Epoch:39, Step:1, loss:2.895\n",
      "perplexity:19.590\n",
      "Epoch:40, Step:1, loss:2.805\n",
      "perplexity:17.994\n",
      "Epoch:41, Step:1, loss:2.730\n",
      "perplexity:15.941\n",
      "Epoch:42, Step:1, loss:2.724\n",
      "perplexity:14.523\n",
      "Epoch:43, Step:1, loss:2.449\n",
      "perplexity:13.373\n",
      "Epoch:44, Step:1, loss:2.395\n",
      "perplexity:12.197\n",
      "Epoch:45, Step:1, loss:2.207\n",
      "perplexity:11.292\n",
      "Epoch:46, Step:1, loss:2.253\n",
      "perplexity:10.271\n",
      "Epoch:47, Step:1, loss:2.131\n",
      "perplexity:9.817\n",
      "Epoch:48, Step:1, loss:2.159\n",
      "perplexity:8.920\n",
      "Epoch:49, Step:1, loss:2.003\n",
      "perplexity:8.347\n",
      "Epoch:50, Step:1, loss:1.874\n",
      "perplexity:7.473\n",
      "Epoch:51, Step:1, loss:1.878\n",
      "perplexity:7.107\n",
      "Epoch:52, Step:1, loss:1.680\n",
      "perplexity:6.439\n",
      "Epoch:53, Step:1, loss:1.678\n",
      "perplexity:5.909\n",
      "Epoch:54, Step:1, loss:1.635\n",
      "perplexity:5.506\n",
      "Epoch:55, Step:1, loss:1.613\n",
      "perplexity:5.206\n",
      "Epoch:56, Step:1, loss:1.551\n",
      "perplexity:4.867\n",
      "Epoch:57, Step:1, loss:1.431\n",
      "perplexity:4.577\n",
      "Epoch:58, Step:1, loss:1.322\n",
      "perplexity:4.231\n",
      "Epoch:59, Step:1, loss:1.271\n",
      "perplexity:3.997\n",
      "Epoch:60, Step:1, loss:1.359\n",
      "perplexity:3.882\n",
      "Epoch:61, Step:1, loss:1.231\n",
      "perplexity:3.472\n",
      "Epoch:62, Step:1, loss:1.192\n",
      "perplexity:3.321\n",
      "Epoch:63, Step:1, loss:1.118\n",
      "perplexity:3.178\n",
      "Epoch:64, Step:1, loss:1.128\n",
      "perplexity:3.062\n",
      "Epoch:65, Step:1, loss:1.025\n",
      "perplexity:2.881\n",
      "Epoch:66, Step:1, loss:0.880\n",
      "perplexity:2.818\n",
      "Epoch:67, Step:1, loss:0.884\n",
      "perplexity:2.668\n",
      "Epoch:68, Step:1, loss:0.896\n",
      "perplexity:2.543\n",
      "Epoch:69, Step:1, loss:0.885\n",
      "perplexity:2.437\n",
      "Epoch:70, Step:1, loss:0.769\n",
      "perplexity:2.323\n",
      "Epoch:71, Step:1, loss:0.833\n",
      "perplexity:2.280\n",
      "Epoch:72, Step:1, loss:0.717\n",
      "perplexity:2.246\n",
      "Epoch:73, Step:1, loss:0.713\n",
      "perplexity:2.145\n",
      "Epoch:74, Step:1, loss:0.660\n",
      "perplexity:2.125\n",
      "Epoch:75, Step:1, loss:0.684\n",
      "perplexity:2.049\n",
      "Epoch:76, Step:1, loss:0.721\n",
      "perplexity:1.960\n",
      "Epoch:77, Step:1, loss:0.632\n",
      "perplexity:1.929\n",
      "Epoch:78, Step:1, loss:0.557\n",
      "perplexity:1.895\n",
      "Epoch:79, Step:1, loss:0.574\n",
      "perplexity:1.845\n",
      "Epoch:80, Step:1, loss:0.614\n",
      "perplexity:1.799\n",
      "Epoch:81, Step:1, loss:0.624\n",
      "perplexity:1.769\n",
      "Epoch:82, Step:1, loss:0.577\n",
      "perplexity:1.699\n",
      "Epoch:83, Step:1, loss:0.497\n",
      "perplexity:1.685\n",
      "Epoch:84, Step:1, loss:0.518\n",
      "perplexity:1.637\n",
      "Epoch:85, Step:1, loss:0.479\n",
      "perplexity:1.599\n",
      "Epoch:86, Step:1, loss:0.405\n",
      "perplexity:1.609\n",
      "Epoch:87, Step:1, loss:0.394\n",
      "perplexity:1.576\n",
      "Epoch:88, Step:1, loss:0.436\n",
      "perplexity:1.547\n",
      "Epoch:89, Step:1, loss:0.425\n",
      "perplexity:1.530\n",
      "Epoch:90, Step:1, loss:0.437\n",
      "perplexity:1.505\n",
      "Epoch:91, Step:1, loss:0.387\n",
      "perplexity:1.479\n",
      "Epoch:92, Step:1, loss:0.313\n",
      "perplexity:1.482\n",
      "Epoch:93, Step:1, loss:0.344\n",
      "perplexity:1.457\n",
      "Epoch:94, Step:1, loss:0.363\n",
      "perplexity:1.436\n",
      "Epoch:95, Step:1, loss:0.398\n",
      "perplexity:1.420\n",
      "Epoch:96, Step:1, loss:0.295\n",
      "perplexity:1.408\n",
      "Epoch:97, Step:1, loss:0.333\n",
      "perplexity:1.381\n",
      "Epoch:98, Step:1, loss:0.288\n",
      "perplexity:1.377\n",
      "Epoch:99, Step:1, loss:0.283\n",
      "perplexity:1.363\n",
      "Epoch:100, Step:1, loss:0.291\n",
      "perplexity:1.336\n"
     ]
    }
   ],
   "source": [
    "net.train(mode=True)\n",
    "for epoch in range(Epoch):\n",
    "    train_loss = 0\n",
    "    for i, (batch_train, batch_label) in enumerate(train_generator):\n",
    "        batch_train = batch_train.type(torch.long)\n",
    "        batch_label = batch_label.type(torch.long)\n",
    "        batch_train = batch_train.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        out, _ = net(batch_train)\n",
    "        loss = criterion(out, batch_label.view(-1))\n",
    "        train_loss +=  loss.item()\n",
    "        if i%100==0:\n",
    "            print('Epoch:{}, Step:{}, loss:{:.3f}'.format(epoch + 1, i + 1, loss.item()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    print('perplexity:{:.3f}'.format(np.exp(train_loss / len(train_generator))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "first = ['巴山上峡重复重', '君不见黄河之水天上来', '细蕊慢逐风', '月落辕门鼓角鸣', '吐谷浑盛强']\n",
    "for begin in first: \n",
    "    begin2num = [char2num[x] for x in begin]\n",
    "    begin2num = torch.from_numpy(np.expand_dims(np.array(begin2num), axis=0)).type(torch.long).to(device)\n",
    "    _, init = net(begin2num)\n",
    "    text_len = 100\n",
    "    input_vector = begin2num[:, -1]\n",
    "    for i in range(text_len):\n",
    "        input_vector = torch.unsqueeze(input_vector, dim=0).type(torch.long).to(device)\n",
    "        out, init = net(input_vector, init)\n",
    "        out = torch.squeeze(out, dim=0)\n",
    "        out = out.cpu()\n",
    "        out = out.detach().numpy()\n",
    "        next_char_index = random_choice(out, k=1)\n",
    "        begin = begin + char_list[next_char_index.item()]\n",
    "        input_vector = torch.Tensor([next_char_index.item()]).type(torch.long)\n",
    "    with open('result.txt', 'a') as f:       \n",
    "        f.write(begin)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
